{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a140b0-ef36-4331-9a95-58ff88d49150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133755c-2f42-47d6-9d21-0b3e94ab75f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4abc0973-2d95-4a4c-86d7-464c89616d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>com1</th>\n",
       "      <th>com2</th>\n",
       "      <th>com3</th>\n",
       "      <th>com4</th>\n",
       "      <th>com5</th>\n",
       "      <th>com6</th>\n",
       "      <th>business_day</th>\n",
       "      <th>tod_afternoon</th>\n",
       "      <th>tod_afternoon_rush</th>\n",
       "      <th>tod_evening</th>\n",
       "      <th>tod_lunch_time</th>\n",
       "      <th>tod_morning</th>\n",
       "      <th>tod_morning_rush</th>\n",
       "      <th>tod_night</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Precipitation</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Relative Humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-15 00:00:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-15 00:15:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-15 00:30:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-15 00:45:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-15 01:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81137</th>\n",
       "      <td>2021-05-31 22:00:00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81138</th>\n",
       "      <td>2021-05-31 22:15:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81139</th>\n",
       "      <td>2021-05-31 22:30:00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81140</th>\n",
       "      <td>2021-05-31 22:45:00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81141</th>\n",
       "      <td>2021-05-31 23:00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81309 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  com1  com2  com3  com4  com5  com6  business_day  \\\n",
       "0     2017-04-15 00:00:00   8.0   5.0   4.0  11.0  15.0   2.0           0.0   \n",
       "1     2017-04-15 00:15:00   1.0   5.0   3.0   8.0  20.0   0.0           0.0   \n",
       "2     2017-04-15 00:30:00   2.0   5.0   6.0   9.0  14.0   0.0           0.0   \n",
       "3     2017-04-15 00:45:00   3.0   3.0   4.0   2.0  13.0   0.0           0.0   \n",
       "4     2017-04-15 01:00:00   7.0   4.0   3.0   6.0  11.0   0.0           0.0   \n",
       "...                   ...   ...   ...   ...   ...   ...   ...           ...   \n",
       "81137 2021-05-31 22:00:00  29.0  18.0  33.0  30.0  77.0  22.0           1.0   \n",
       "81138 2021-05-31 22:15:00  11.0   9.0  15.0  15.0  40.0   9.0           1.0   \n",
       "81139 2021-05-31 22:30:00  21.0  32.0  33.0  24.0  60.0   6.0           1.0   \n",
       "81140 2021-05-31 22:45:00  21.0  29.0  27.0  39.0  55.0   9.0           1.0   \n",
       "81141 2021-05-31 23:00:00  11.0  43.0  30.0  33.0  64.0   6.0           1.0   \n",
       "\n",
       "       tod_afternoon  tod_afternoon_rush  tod_evening  tod_lunch_time  \\\n",
       "0                0.0                 0.0          0.0             0.0   \n",
       "1                0.0                 0.0          0.0             0.0   \n",
       "2                0.0                 0.0          0.0             0.0   \n",
       "3                0.0                 0.0          0.0             0.0   \n",
       "4                0.0                 0.0          0.0             0.0   \n",
       "...              ...                 ...          ...             ...   \n",
       "81137            0.0                 0.0          1.0             0.0   \n",
       "81138            0.0                 0.0          1.0             0.0   \n",
       "81139            0.0                 0.0          1.0             0.0   \n",
       "81140            0.0                 0.0          1.0             0.0   \n",
       "81141            0.0                 0.0          1.0             0.0   \n",
       "\n",
       "       tod_morning  tod_morning_rush  tod_night  Temperature  Precipitation  \\\n",
       "0              0.0               0.0        1.0          7.3            0.0   \n",
       "1              0.0               0.0        1.0          7.3            0.0   \n",
       "2              0.0               0.0        1.0          7.3            0.0   \n",
       "3              0.0               0.0        1.0          7.3            0.0   \n",
       "4              0.0               0.0        1.0          6.3            0.0   \n",
       "...            ...               ...        ...          ...            ...   \n",
       "81137          0.0               0.0        0.0         13.5            0.0   \n",
       "81138          0.0               0.0        0.0         13.5            0.0   \n",
       "81139          0.0               0.0        0.0         13.5            0.0   \n",
       "81140          0.0               0.0        0.0         13.5            0.0   \n",
       "81141          0.0               0.0        0.0         12.9            0.0   \n",
       "\n",
       "       Wind Speed  Relative Humidity  \n",
       "0             9.0               57.0  \n",
       "1             9.0               57.0  \n",
       "2             9.0               57.0  \n",
       "3             9.0               57.0  \n",
       "4             7.0               58.0  \n",
       "...           ...                ...  \n",
       "81137        13.0               74.0  \n",
       "81138        13.0               74.0  \n",
       "81139        13.0               74.0  \n",
       "81140        13.0               74.0  \n",
       "81141         7.0               74.0  \n",
       "\n",
       "[81309 rows x 19 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/\"\n",
    "bixi_dat = pd.read_csv(path+\"bixi_wrt_cal_15min.csv\", parse_dates = ['index'],  index_col=0)\n",
    "bixi_dat.rename(columns={'index':'timestamp'}, inplace=True)\n",
    "bixi_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00fe4cce-0f51-435a-ad4f-c6bdf4ca74c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81309, 6)\n",
      "(81309, 12)\n"
     ]
    }
   ],
   "source": [
    "# Bike-sharing demand for each community \n",
    "demand_features = ['com1', 'com2', 'com3', 'com4', 'com5', 'com6']\n",
    "# Weather-calender variables\n",
    "extra_features = ['business_day', 'tod_afternoon', 'tod_afternoon_rush','tod_evening','tod_lunch_time','tod_morning','tod_morning_rush','tod_night','Temperature','Precipitation', 'Wind Speed', 'Relative Humidity']                        \n",
    "\n",
    "# Bike sharing demand for each community over time\n",
    "bixi_demand = bixi_dat[demand_features].values\n",
    "print(demand_dat.shape)\n",
    "# Weather-calender variables over time\n",
    "weather_calender = bixi_dat[extra_features].values\n",
    "print(weather_calender.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7b8d3-6dc5-4d35-9a7b-35454bd7f325",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea7e5161-696a-4644-a366-77fa962343c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sequence(sequence, lag):\n",
    "    '''\n",
    "    This function splits a given univariate sequence into\n",
    "    multiple samples where each sample has a specified number \n",
    "    of time steps and the output is a single time step.\n",
    "    '''\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "    # find the end of this pattern\n",
    "        end_ix = i + lag\n",
    "\n",
    "    # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "    # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def convert_to_supervised(lag, dat1, dat2=None):\n",
    "    '''    \n",
    "    This function takes a 2D sequennce, scales the array and splits \n",
    "    a given multivariate sequence into multiple samples where each sample has a specified number \n",
    "    of time steps. It returns multiple time steps as well as the scaler.\n",
    "    param dat1: Bike sharing demand for each community over time\n",
    "    param dat2: Weather-calender variables over time\n",
    "    '''\n",
    "    _, start_test = train_val_test()\n",
    "    if dat2 is None:\n",
    "        # scale data to [-1, 1]\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        dat1_train, dat1_test = train_test_split(X=dat1, start_test = start_test)\n",
    "        scaled_train = scaler.fit_transform(dat1_train)\n",
    "        scaled_test = scaler.transform(dat1_test)\n",
    "        scaled_dat = np.vstack([scaled_train, scaled_test])\n",
    "        m, n = scaled_dat.shape\n",
    "        # e.g., if lag = 7, BIXI demand of past 7*15 minutes\n",
    "        scaled_X = np.zeros((m-lag,lag, n))\n",
    "        scaled_y = np.zeros((m-lag,n))\n",
    "        \n",
    "        for i in range(0,n):\n",
    "            X, y = split_sequence(scaled_dat[:,i],lag)\n",
    "            scaled_X[:,:,i] = X\n",
    "            scaled_y[:,i] = y\n",
    "        return scaler, scaled_X, scaled_y\n",
    "    \n",
    "    else:        \n",
    "        scaler1 = MinMaxScaler(feature_range=(-1, 1))        \n",
    "        dat1_train, dat1_test = train_test_split(X=dat1, start_test = start_test)        \n",
    "        scaled_train1 = scaler1.fit_transform(dat1_train)\n",
    "        scaled_test1 = scaler1.transform(dat1_test)\n",
    "        \n",
    "        scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
    "        dat2_train, dat2_test = train_test_split(X=dat2, start_test = start_test)\n",
    "        scaled_train2 = scaler2.fit_transform(dat2_train)\n",
    "        scaled_test2 = scaler2.transform(dat2_test)\n",
    "\n",
    "        scaled_dat1 = np.vstack([scaled_train1, scaled_test1])\n",
    "        scaled_dat2 = np.vstack([scaled_train2, scaled_test2])\n",
    "\n",
    "        m, n = scaled_dat1.shape\n",
    "        _, k = scaled_dat2.shape\n",
    "\n",
    "        # Bixi demand of the past lag days\n",
    "        scaled_X = np.zeros((m-lag, lag+k, n))\n",
    "        scaled_y = np.zeros((m-lag,n))\n",
    "\n",
    "        for i in range(0,n):\n",
    "            X,y = split_sequence(scaled_dat1[:,i],lag)\n",
    "            X = np.hstack([X,scaled_dat2[lag:,:]])\n",
    "            scaled_X[:,:,i] = X\n",
    "            scaled_y[:,i] = y\n",
    "        return scaler1, scaler2, scaled_X, scaled_y\n",
    "\n",
    "def invert_scale(scaler, y):\n",
    "    '''\n",
    "    Inverse scaling for a predicted value\n",
    "    '''\n",
    "    return scaler.inverse_transform(y)\n",
    "\n",
    "def train_val_test(dat=bixi_dat, train_end_date='2020-04-15 10:45:00', val_end_date='2020-07-15 18:15:00'):\n",
    "    '''\n",
    "    Important: determine train, test and validation based on Covid outbreak\n",
    "    param train_end (timstamp): end of training set\n",
    "    param val_end (timestamp): end of validation set\n",
    "    '''\n",
    "    assert train_end_date < val_end_date, \"WARNING: validation date must be later than training date.\"\n",
    "    train = dat.timestamp[dat.timestamp == train_end_date].index.values[0]   \n",
    "    validation = dat.timestamp[dat.timestamp == val_end_date].index.values[0]   \n",
    "    train_val_ratio = validation/train   \n",
    "    # test set preparation    \n",
    "    start_test =  len(dat) - validation\n",
    "\n",
    "    print(f\"Train set starts at {dat.loc[0,'timestamp']} and ends at {dat.loc[train,'timestamp']}\")\n",
    "    print(f\"Validation set starts at {dat.loc[train,'timestamp']} and ends at {dat.loc[validation,'timestamp']}\")\n",
    "    print(f\"Test set starts at {dat.loc[validation,'timestamp']} and ends at {dat.loc[len(dat),'timestamp']}\")\n",
    "    return train_val_ratio, start_test\n",
    "\n",
    "def train_test_split(X, y = None, start_test = None, ratio= 0.3):\n",
    "    '''\n",
    "    Train test split for times-series data\n",
    "    '''\n",
    "    if start_test is None:\n",
    "        lenght,_ = X.shape\n",
    "        start_test = int(ratio*lenght)\n",
    "\n",
    "    X_train, X_test = X[0:-start_test], X[-start_test:] \n",
    "    print(\"X training size: \", X_train.shape)\n",
    "    print(\"X test size: \",X_test.shape )\n",
    "    if y is None:\n",
    "        return  X_train, X_test  \n",
    "    \n",
    "    else:\n",
    "        y_train, y_test = y[:-start_test], y[-start_test:]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "def model_dim(X):\n",
    "    \"\"\"\n",
    "    Deep learning architucture size for input, fearures and output\n",
    "    \"\"\"\n",
    "    n_features, n_steps_out = X.shape[2], X.shape[2]\n",
    "    n_steps_in = X.shape[1]\n",
    "    return n_steps_in, n_features, n_steps_out\n",
    "\n",
    "\n",
    "def integer_factor(n):\n",
    "    \"\"\" calculate integer factorization of n\n",
    "    and then returns them as two multiplications\"\"\"\n",
    "    def is_prime(n):\n",
    "        if n == 1:\n",
    "            return False\n",
    "        if n % 2 == 0:\n",
    "            return False\n",
    "        i = 3\n",
    "        while i * i <= n:\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "            i += 2\n",
    "        return True\n",
    "\n",
    "    def prime_factors(n):\n",
    "        prime_factor_list = []\n",
    "        prime_factor_list.append(1)\n",
    "        for i in itertools.chain([2], itertools.count(3, 2)):\n",
    "            if n <= 1:\n",
    "                break\n",
    "            while n % i == 0:\n",
    "                n //= i\n",
    "                prime_factor_list.append(i)\n",
    "        return prime_factor_list\n",
    "\n",
    "    lst = prime_factors(n)\n",
    "    lng = len(lst)\n",
    "    half= int(np.round(lng/2+1))\n",
    "    list1, list2 = lst[0:half], lst[half:]\n",
    "    n_steps, n_sequence = np.prod(list1), np.prod(list2)\n",
    "    return n_steps, n_sequence\n",
    "\n",
    "\n",
    "def demand_extra_feature(dat):\n",
    "    '''\n",
    "    This function adds engineered features to demand dataset.\n",
    "    '''    \n",
    "    Mean = dat.mean(1)\n",
    "    Max = dat.max(1)\n",
    "    Min = dat.min(1)\n",
    "    Q25 = np.quantile(dat,0.25,axis=1)\n",
    "    Q50 = np.quantile(dat,0.50,axis=1)\n",
    "    Q75 = np.quantile(dat,0.75,axis=1)\n",
    "    Std = dat.std(axis=1)\n",
    "    return np.vstack([Mean,Max,Min,Q25,Q50,Q75,Std]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6ee35-c275-4a3c-b14a-1d18794777fd",
   "metadata": {},
   "source": [
    "### Deep Learning Models Archituctures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba5af2-7442-46b2-82a0-febf0cfdd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM family\n",
    "def one_LSTM(n_steps_in, n_features, n_steps_out, lstm_size, dropout, dc_size):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(n_steps_in, n_features)))\n",
    "    model.add(LSTM(lstm_size, dropout=dropout, recurrent_dropout=0, activation='tanh', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dc_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',metrics=['MSE'])\n",
    "    return model\n",
    "\n",
    "def one_biLSTM(n_steps_in, n_features, n_steps_out, lstm_size, dropout, dc_size):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(n_steps_in, n_features)))\n",
    "    model.add(Bidirectional(LSTM(lstm_size, dropout=dropout, recurrent_dropout=0, activation='tanh', return_sequences=True)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dc_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',metrics=['MSE'])\n",
    "    return model\n",
    "\n",
    "## CNN-LSTM family\n",
    "def TreNet_LSTM(n_steps_in, n_features, n_steps_out, filters, lstm_size, dropout, dc_size):\n",
    "    n_steps, n_seq =  integer_factor(n_steps_in)\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=filters, kernel_size=1),\n",
    "                          input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=filters, kernel_size=1, activation='relu')))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(lstm_size, activation='tanh', recurrent_dropout=0))\n",
    "    model.add(Dense(dc_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',metrics=['MSE'])\n",
    "    return model\n",
    "\n",
    "def TreNet_biLSTM(n_steps_in, n_features, n_steps_out, filters, lstm_size, dropout, dc_size):\n",
    "    n_steps, n_seq =  integer_factor(n_steps_in)\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=filters, kernel_size=1),\n",
    "                          input_shape=(None, n_steps, n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=filters, kernel_size=1, activation='relu')))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Bidirectional(LSTM(lstm_size, dropout=dropout, recurrent_dropout=0, activation='tanh')))\n",
    "    model.add(Dense(dc_size))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',metrics=['MSE'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7cd55c-813f-4858-8ec2-5d4fb44f2f44",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc044d86-fcd9-4204-922d-8e3fc541829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(history,label2='Validation Loss'):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label=label2)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(linestyle=\"--\")\n",
    "    plt.show();\n",
    "\n",
    "def evaluate_forecasts(actual, predicted, text = \"Test\", plot=True):\n",
    "    \"\"\"\n",
    "    Evaluate prediction performance based on RMSE and MAE\n",
    "    \"\"\"\n",
    "    RMSEs = list()\n",
    "    MAEs = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = np.sqrt(mse)\n",
    "        # store\n",
    "        RMSEs.append(rmse)\n",
    "\n",
    "        # calculate mae\n",
    "        mae = mean_absolute_error(actual[:,i], predicted[:,i])\n",
    "        # store\n",
    "        MAEs.append(mae)\n",
    "\n",
    "    # calculate overall RMSE and MAE\n",
    "    y_true = actual.flatten()\n",
    "    y_hat = predicted.flatten()\n",
    "\n",
    "    overal_mae = mean_absolute_error(y_true, y_hat)\n",
    "    overal_rmse = np.sqrt(mean_squared_error(y_true, y_hat))\n",
    "\n",
    "    print(\"#### Evaluating performance metrics ####\")\n",
    "    print(\"\\n====\"+ text+\" SET ====\")\n",
    "    print(\"MAE: {0:.3f}\".format(overal_mae))\n",
    "    print(\"RMSE: {0:.3f}\".format(overal_rmse))\n",
    "    print(\"MAEs: \", np.round(MAEs,3))\n",
    "    print(\"RMSEs: \", np.round(RMSEs,3))\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(np.arange(len(RMSEs)), RMSEs, label=True)\n",
    "        plt.plot(np.arange(len(MAEs)), MAEs, label=True)\n",
    "        plt.grid(linestyle=\"--\")\n",
    "        plt.xlabel(\"Community number\")\n",
    "        plt.legend([\"RMSE\", \"MAE\"])\n",
    "        plt.title(\"Performance metrics for \"+ text +\" dataset\")\n",
    "        plt.show()\n",
    "\n",
    "    return overal_mae, MAEs, overal_rmse, RMSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c18cbf-8b33-4ff4-9f79-7b2ebc30bbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
